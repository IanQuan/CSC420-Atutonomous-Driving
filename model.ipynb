{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63a9a5d27495f4b3",
      "metadata": {
        "id": "63a9a5d27495f4b3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Activation, MaxPool2D, Concatenate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "from keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
        "import time\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.metrics import MeanIoU\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NtcNG54flUjV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtcNG54flUjV",
        "outputId": "b4ea14a6-54f9-4184-fb30-4884bc55ff8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MlB1Cjo6NCim",
      "metadata": {
        "id": "MlB1Cjo6NCim"
      },
      "outputs": [],
      "source": [
        "!unzip \"/content/test.zip\"\n",
        "!unzip \"/content/train.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Il_fIR1OlCfv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Il_fIR1OlCfv",
        "outputId": "8c76eb7a-3a04-42ab-dc7c-f44451e72aa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 135 stereo image pairs with calibration data and masks.\n"
          ]
        }
      ],
      "source": [
        "def extract_focal_and_baseline(calib_file):\n",
        "    with open(calib_file, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        P0 = np.array([float(x) for x in lines[0].strip().split()[1:]])\n",
        "        P1 = np.array([float(x) for x in lines[1].strip().split()[1:]])\n",
        "        f = P0[0]\n",
        "        b = (P1[3] - P0[3]) / f\n",
        "    return f, abs(b)\n",
        "\n",
        "def load_data(img_left_folder, img_right_folder, calib_folder, mask_folder):\n",
        "    images_left_files = [f for f in os.listdir(img_left_folder) if f.endswith(('.png', '.jpg'))]\n",
        "    images_left_files.sort()\n",
        "\n",
        "    stereo_images = []\n",
        "    for filename in images_left_files:\n",
        "        left_image_path = os.path.join(img_left_folder, filename)\n",
        "        right_image_path = os.path.join(img_right_folder, filename)\n",
        "\n",
        "        calib_filename = filename.replace('.jpg', '.txt').replace('.png', '.txt')\n",
        "        calib_file_path = os.path.join(calib_folder, calib_filename)\n",
        "\n",
        "        parts = filename.split('_')\n",
        "        mask_filename = parts[0] + '_road_' + '_'.join(parts[1:])\n",
        "        mask_filename = mask_filename.replace('.jpg', '.png')\n",
        "        mask_path = os.path.join(mask_folder, mask_filename)\n",
        "\n",
        "        if os.path.exists(right_image_path) and os.path.exists(calib_file_path) and os.path.exists(mask_path):\n",
        "            image_left = cv2.imread(left_image_path, cv2.IMREAD_COLOR)\n",
        "            image_right = cv2.imread(right_image_path, cv2.IMREAD_COLOR)\n",
        "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Load the mask\n",
        "\n",
        "            # Resize images and mask\n",
        "            image_left = cv2.resize(image_left, (640, 192), interpolation=cv2.INTER_LINEAR)\n",
        "            image_right = cv2.resize(image_right, (640, 192), interpolation=cv2.INTER_LINEAR)\n",
        "            mask = cv2.resize(mask, (640, 192), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "            if image_left is not None and image_right is not None and mask is not None:\n",
        "                f, b = extract_focal_and_baseline(calib_file_path)\n",
        "                stereo_images.append((image_left, image_right, mask, f, b))\n",
        "            else:\n",
        "                print(f\"Error loading files for {filename}\")\n",
        "        else:\n",
        "            if not os.path.exists(right_image_path):\n",
        "                print(f\"Warning: No corresponding right image found for {filename} in {img_right_folder}\")\n",
        "            if not os.path.exists(calib_file_path):\n",
        "                print(f\"Warning: No corresponding calibration file found for {calib_filename} in {calib_folder}\")\n",
        "            if not os.path.exists(mask_path):\n",
        "                print(f\"Warning: No corresponding mask found for {mask_filename} in {mask_folder}\")\n",
        "    return stereo_images\n",
        "\n",
        "img_left_folder = \"/content/train/image_left\"\n",
        "img_right_folder = \"/content/train/image_right\"\n",
        "calib_folder = \"/content/train/calib\"\n",
        "mask_folder = \"/content/train/gt_image_left\"\n",
        "\n",
        "stereo_images = load_data(img_left_folder, img_right_folder, calib_folder, mask_folder)\n",
        "print(f\"Loaded {len(stereo_images)} stereo image pairs with calibration data and masks.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d303017f43bbd4df",
      "metadata": {
        "collapsed": false,
        "id": "d303017f43bbd4df"
      },
      "source": [
        "## 1. Compute disparity between the two stereo images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cnDRGlBqR_E2",
      "metadata": {
        "id": "cnDRGlBqR_E2"
      },
      "outputs": [],
      "source": [
        "# Experiment the best combination of numDisparities and blockSize\n",
        "\n",
        "img_left, img_right = stereo_images[0][0], stereo_images[0][1]  # Test on the first pair to start\n",
        "\n",
        "# Convert images to grayscale\n",
        "img_left_gray = cv2.cvtColor(img_left, cv2.COLOR_BGR2GRAY)\n",
        "img_right_gray = cv2.cvtColor(img_right, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "numDisparities_values = [64, 80, 96, 112]  # numDisparities must be divisible by 16\n",
        "blockSizes = [21, 23, 25, 27, 29]  # block sizes to test\n",
        "\n",
        "# Iterate over each combination of numDisparities and blockSize\n",
        "for nd in numDisparities_values:\n",
        "    for bs in blockSizes:\n",
        "        stereo = cv2.StereoBM_create(numDisparities=nd, blockSize=bs)\n",
        "        disparity = stereo.compute(img_left_gray, img_right_gray)\n",
        "\n",
        "        # Plot the result\n",
        "        plt.figure(figsize=(10, 3))\n",
        "        plt.imshow(disparity, 'plasma')\n",
        "        plt.title(f'numDisparities={nd}, blockSize={bs}')\n",
        "        plt.colorbar()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ESORf6aKq0u",
      "metadata": {
        "id": "5ESORf6aKq0u"
      },
      "outputs": [],
      "source": [
        "# Plot the disparity map for the first 5 stereo images\n",
        "for i, (img_left, img_right, mask, f, b) in enumerate(stereo_images[:5]):\n",
        "    if img_left is not None and img_right is not None:\n",
        "        plt.figure(figsize=(21, 7))\n",
        "        plt.subplot(131)\n",
        "        plt.imshow(cv2.cvtColor(img_left, cv2.COLOR_BGR2RGB))\n",
        "        plt.title('Left Image')\n",
        "\n",
        "        plt.subplot(132)\n",
        "        plt.imshow(cv2.cvtColor(img_right, cv2.COLOR_BGR2RGB))\n",
        "        plt.title('Right Image')\n",
        "\n",
        "        img_left_gray = cv2.cvtColor(img_left, cv2.COLOR_BGR2GRAY)\n",
        "        img_right_gray = cv2.cvtColor(img_right, cv2.COLOR_BGR2GRAY)\n",
        "        stereo = cv2.StereoBM_create(numDisparities=96, blockSize=21)\n",
        "        disparity = stereo.compute(img_left_gray, img_right_gray).astype(np.float32) / 16.0\n",
        "        plt.subplot(133)\n",
        "        plt.imshow(disparity, 'gray')\n",
        "        plt.title('Disparity Map')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Skipping image pair {i} due to loading error.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7ae891b92d11f99",
      "metadata": {
        "collapsed": false,
        "id": "e7ae891b92d11f99"
      },
      "source": [
        "## 2. Compute depth of each pixel. Compute 3D location of each pixel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bKyjV1YFMjKl",
      "metadata": {
        "id": "bKyjV1YFMjKl"
      },
      "outputs": [],
      "source": [
        "def compute_depth_map(img_left, img_right, f, b):\n",
        "    img_left_gray = cv2.cvtColor(img_left, cv2.COLOR_BGR2GRAY)\n",
        "    img_right_gray = cv2.cvtColor(img_right, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    stereo = cv2.StereoBM_create(numDisparities=96, blockSize=21)\n",
        "    disparity = stereo.compute(img_left_gray, img_right_gray).astype(np.float32) / 16.0\n",
        "\n",
        "    disparity[disparity == 0] = 0.1  # To avoid division by zero\n",
        "    depth = f * b / disparity\n",
        "\n",
        "    # Normalize depth to span 0 to 255\n",
        "    # Convert depth values from meters to a scale of 0-255 for display purposes\n",
        "    max_depth = 50\n",
        "    min_depth = 0\n",
        "    depth_normalized = 255 * (depth - min_depth) / (max_depth - min_depth)\n",
        "\n",
        "    # Clip values to ensure they remain within [0, 255]\n",
        "    depth_normalized = np.clip(depth_normalized, 0, 255)\n",
        "\n",
        "    # Convert to an 8-bit unsigned integer\n",
        "    depth_normalized = depth_normalized.astype(np.uint8)\n",
        "\n",
        "    return depth_normalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IPwqykjnlI6y",
      "metadata": {
        "id": "IPwqykjnlI6y"
      },
      "outputs": [],
      "source": [
        "# Compute and display depth maps for the first few stereo image pairs\n",
        "for img_left, img_right, mask, f, b in stereo_images[:5]:\n",
        "    if img_left is not None and img_right is not None:\n",
        "        plt.figure(figsize=(21, 7))\n",
        "        plt.subplot(131)\n",
        "        plt.imshow(cv2.cvtColor(img_left, cv2.COLOR_BGR2RGB))\n",
        "        plt.title('Left Image')\n",
        "\n",
        "        plt.subplot(132)\n",
        "        plt.imshow(cv2.cvtColor(img_right, cv2.COLOR_BGR2RGB))\n",
        "        plt.title('Right Image')\n",
        "\n",
        "        plt.subplot(133)\n",
        "        depth = compute_depth_map(img_left, img_right, f, b)\n",
        "        depth_display = plt.imshow(depth, cmap='viridis')\n",
        "        plt.title('Depth Map')\n",
        "        plt.colorbar(depth_display, orientation='vertical', shrink=0.6)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Skipping image pair {i} due to loading error.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33c7a345025a957",
      "metadata": {
        "collapsed": false,
        "id": "33c7a345025a957"
      },
      "source": [
        "## 3. Train a road classifier on a set of annotated images, and compute road pixels in your image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MTOkda59BLxR",
      "metadata": {
        "id": "MTOkda59BLxR"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OFvIsOC5I9aW",
      "metadata": {
        "id": "OFvIsOC5I9aW"
      },
      "outputs": [],
      "source": [
        "def compute_and_fuse_depth(stereo_images):\n",
        "    training_data = []\n",
        "    for idx, (img_left, img_right, mask, f, b) in enumerate(stereo_images):\n",
        "        # Compute the depth map\n",
        "        depth_map = compute_depth_map(img_left, img_right, f, b)\n",
        "\n",
        "        # Normalize the mask and convert to float\n",
        "        mask_normalized = mask.astype(float) / 255.0  # Assuming mask in 0-255 range\n",
        "\n",
        "        # Normalize depth to range [0,1] if depth_map values are 0-255\n",
        "        # If depth values are not in 0-255 range, adjust normalization accordingly\n",
        "        depth_normalized = (depth_map / 255.0)\n",
        "        inverted_depth = 1.0 - depth_normalized\n",
        "\n",
        "        # Create a fused depth map where we only apply the mask where depth_map values are non-zero\n",
        "        # and mask is white (mask_normalized == 1)\n",
        "        fused_depth = np.where(mask_normalized == 1, inverted_depth, 0)\n",
        "\n",
        "        training_data.append((img_left, depth_normalized, mask_normalized, fused_depth))\n",
        "\n",
        "    return training_data\n",
        "training_data = compute_and_fuse_depth(stereo_images)\n",
        "training_data.pop(0) # the mask of the first image is not well defined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f8ufOHgJIdY",
      "metadata": {
        "id": "6f8ufOHgJIdY"
      },
      "outputs": [],
      "source": [
        "# Sanity check for training data\n",
        "def plot_training_data(training_data):\n",
        "    num_samples = 5\n",
        "    fig, axs = plt.subplots(num_samples, 4, figsize=(20, num_samples * 3))\n",
        "\n",
        "    for idx, (original, depth_map, mask, fused_depth) in enumerate(training_data):\n",
        "        if idx < num_samples:\n",
        "            # Original Image\n",
        "            ax_original = axs[idx, 0]\n",
        "            im_original = ax_original.imshow(cv2.cvtColor(original, cv2.COLOR_BGR2RGB))\n",
        "            ax_original.set_title('Original Image')\n",
        "            ax_original.axis('off')  # Hide axes ticks\n",
        "            fig.colorbar(im_original, ax=ax_original, orientation='vertical')\n",
        "\n",
        "            # Depth Map\n",
        "            ax_depth = axs[idx, 1]\n",
        "            im_depth = ax_depth.imshow(depth_map, cmap='gray')\n",
        "            ax_depth.set_title('Depth Map')\n",
        "            ax_depth.axis('off')\n",
        "            fig.colorbar(im_depth, ax=ax_depth, orientation='vertical')\n",
        "\n",
        "            # Ground Truth Mask\n",
        "            ax_mask = axs[idx, 2]\n",
        "            im_mask = ax_mask.imshow(mask, cmap='gray')\n",
        "            ax_mask.set_title('Ground Truth Mask')\n",
        "            ax_mask.axis('off')\n",
        "            fig.colorbar(im_mask, ax=ax_mask, orientation='vertical')\n",
        "\n",
        "            # Fused Depth Map\n",
        "            ax_fused = axs[idx, 3]\n",
        "            im_fused = ax_fused.imshow(fused_depth, cmap='gray')\n",
        "            ax_fused.set_title('Fused Depth Map')\n",
        "            ax_fused.axis('off')\n",
        "            fig.colorbar(im_fused, ax=ax_fused, orientation='vertical')\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "plot_training_data(training_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QZiXu7Y6LuJv",
      "metadata": {
        "id": "QZiXu7Y6LuJv"
      },
      "outputs": [],
      "source": [
        "X = []  # Images\n",
        "y = []  # Fused masks\n",
        "for (img_left, depth_map, mask, fused_depth) in training_data:\n",
        "    X.append(img_to_array(img_left))\n",
        "    y.append(img_to_array(fused_depth))\n",
        "\n",
        "\n",
        "# Convert lists to numpy arrays and normalize image data\n",
        "X = np.array(X, dtype=np.float32) / 255.0\n",
        "y = np.array(y, dtype=np.float32)\n",
        "\n",
        "\n",
        "# Splitting the data into 80% for training + validation and 20% for testing\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "train_data = (X_train, y_train)\n",
        "validation_data = (X_val, y_val)\n",
        "\n",
        "IMG_HEIGHT = X.shape[1]\n",
        "IMG_WIDTH  = X.shape[2]\n",
        "IMG_CHANNELS = X.shape[3]\n",
        "input_shape = (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
        "\n",
        "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n",
        "print(input_shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n2Xge6nmR6DV",
      "metadata": {
        "id": "n2Xge6nmR6DV"
      },
      "outputs": [],
      "source": [
        "# Sanity check for training data\n",
        "def plot_training_data(X, y, num_samples=5):\n",
        "    plt.figure(figsize=(10, num_samples * 2))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        if i >= len(X):\n",
        "            break\n",
        "        ax = plt.subplot(num_samples, 2, 2*i+1)\n",
        "        plt.imshow(X[i])\n",
        "        ax.set_title('Original Image')\n",
        "        ax.axis('off')\n",
        "\n",
        "        ax = plt.subplot(num_samples, 2, 2*i+2)\n",
        "        plt.imshow(y[i].squeeze(), cmap='gray')\n",
        "        ax.set_title('Ground Truth')\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_data(train_data[0], train_data[1], num_samples=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IYx5RPf6lOEX",
      "metadata": {
        "id": "IYx5RPf6lOEX"
      },
      "outputs": [],
      "source": [
        "# Building Unet by dividing encoder and decoder into blocks\n",
        "def conv_block(input, num_filters):\n",
        "    conv_initializer = \"he_normal\"\n",
        "\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\", kernel_initializer=conv_initializer)(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\", kernel_initializer=conv_initializer)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "# Encoder block: Conv block followed by max-pooling\n",
        "def encoder_block(input, num_filters):\n",
        "    # each conv_block performs 2 conv operations then max-pooling with size 2\n",
        "    s = conv_block(input, num_filters)  # s will be used as skip_features for concatenation to the decoder\n",
        "    p = MaxPool2D((2, 2))(s)  # p will be used as the input of the next encoder block\n",
        "    return s, p\n",
        "\n",
        "\n",
        "# Decoder block\n",
        "# skip features gets input from encoder for concatenation\n",
        "\n",
        "# Concatenating feature maps allows the network to use fine-grained details captured in the encoder alongside the more\n",
        "# abstract features generated in the decoder. This combination is vital for accurate localization and detailed\n",
        "# segmentation because it brings together context (from deeper layers) and localization (from earlier layers).\n",
        "def decoder_block(input, skip_features, num_filters):\n",
        "    d = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)  # up-sampling\n",
        "    d = Concatenate()(\n",
        "        [d, skip_features])  # merges the up-sampled feature maps with the corresponding feature maps from the encoder\n",
        "    d = conv_block(d, num_filters)  # process the concatenated feature maps to refine features further\n",
        "    return d\n",
        "\n",
        "\n",
        "# Build Unet using the blocks\n",
        "def build_unet(input_shape):\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    # s1 is the output of the first conv_block ( skip_features that will later be concatenated with the corresponding decoder layer's output)\n",
        "    # p1 is the output after the max-pooling layer following s1 (input to the next encoder block)\n",
        "    s1, p1 = encoder_block(inputs, 64)\n",
        "    s2, p2 = encoder_block(p1, 128)\n",
        "    s3, p3 = encoder_block(p2, 256)\n",
        "    s4, p4 = encoder_block(p3, 512)\n",
        "\n",
        "    b = conv_block(p4, 1024)  # Bridge\n",
        "\n",
        "    d1 = decoder_block(b, s4, 512)\n",
        "    d2 = decoder_block(d1, s3, 256)\n",
        "    d3 = decoder_block(d2, s2, 128)\n",
        "    d4 = decoder_block(d3, s1, 64)\n",
        "\n",
        "    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)  # Binary (can be multiclass)\n",
        "\n",
        "    model = Model(inputs, outputs, name=\"U-Net\")\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z_mnyoehX4Dn",
      "metadata": {
        "id": "z_mnyoehX4Dn"
      },
      "outputs": [],
      "source": [
        "# Perform data augmentation by flipping the image horizontally\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# Create an instance of ImageDataGenerator with horizontal flipping enabled\n",
        "data_gen_args = dict(horizontal_flip=True,\n",
        "                     fill_mode='nearest')  # fill_mode is used to fill in new pixels that can appear after a transformation\n",
        "\n",
        "# Instantiate the ImageDataGenerator\n",
        "image_datagen = ImageDataGenerator(**data_gen_args)\n",
        "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
        "\n",
        "\n",
        "# Provide the same seed to the flow methods to ensure the transformations are the same for images and masks\n",
        "seed = 1\n",
        "batch_size = 16\n",
        "\n",
        "# Prepare the generator for the images and masks\n",
        "image_generator = image_datagen.flow(X_train, batch_size=batch_size, shuffle=True, seed=seed)\n",
        "mask_generator = mask_datagen.flow(y_train, batch_size=batch_size, shuffle=True, seed=seed)\n",
        "\n",
        "val_image_generator = image_datagen.flow(X_val, batch_size=batch_size, shuffle=True, seed=seed)\n",
        "val_mask_generator = mask_datagen.flow(y_val, batch_size=batch_size, shuffle=True, seed=seed)\n",
        "\n",
        "# Combine generators into one which yields image and masks\n",
        "train_generator = zip(image_generator, mask_generator)\n",
        "val_generator = zip(val_image_generator, val_mask_generator)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G2ugs3U1LB7n",
      "metadata": {
        "id": "G2ugs3U1LB7n"
      },
      "outputs": [],
      "source": [
        "# Loss function\n",
        "smooth = 1.\n",
        "\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return -dice_coef(y_true, y_pred)\n",
        "\n",
        "def intersection_over_union(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    union = K.sum(y_true_f) + K.sum(y_pred_f) - intersection\n",
        "    return intersection / union"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lvkyxlKEhX2g",
      "metadata": {
        "id": "lvkyxlKEhX2g"
      },
      "outputs": [],
      "source": [
        "def unet_training(train_generator, validation_data, input_shape, batch_size, epochs):\n",
        "\n",
        "    # Set up the model\n",
        "    timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "    tensorboard_callback = TensorBoard(log_dir=f'logs/{timestamp}', histogram_freq=1)\n",
        "    early_stopping_callback = EarlyStopping(monitor='val_dice_coef', patience=10, verbose=1, restore_best_weights=True)\n",
        "\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    model = build_unet(input_shape)  # Update input shape to match image size\n",
        "    model.compile(optimizer=optimizer, loss=dice_coef_loss, metrics=['accuracy', dice_coef, intersection_over_union])\n",
        "    # model.summary()\n",
        "\n",
        "\n",
        "    # Train the model using the data generator\n",
        "    history = model.fit(\n",
        "        train_data[0],\n",
        "        train_data[1],\n",
        "        validation_data=validation_data,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        shuffle=True,\n",
        "        verbose=1,\n",
        "        steps_per_epoch=len(X_train) // batch_size,\n",
        "        callbacks=[tensorboard_callback, early_stopping_callback]\n",
        "    )\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'], loc='upper left')\n",
        "    plt.show()\n",
        "    return model\n",
        "\n",
        "\n",
        "model = unet_training(  train_generator,\n",
        "                        validation_data,\n",
        "                        input_shape,\n",
        "                        batch_size=8,\n",
        "                        epochs=15,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mL2RDbW_BUYP",
      "metadata": {
        "id": "mL2RDbW_BUYP"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir /content/logs\n",
        "tensorboard_callback = TensorBoard(log_dir='/content/drive/My Drive/Colab Notebooks/logs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WY0a2JLxJX3S",
      "metadata": {
        "id": "WY0a2JLxJX3S"
      },
      "outputs": [],
      "source": [
        "# Evaluate Model\n",
        "\n",
        "def load_data(test_image_folder):\n",
        "    image_files = [f for f in os.listdir(test_image_folder) if f.endswith(('.png', '.jpg'))]\n",
        "    image_files.sort()\n",
        "\n",
        "    test_images = []\n",
        "    for filename in image_files:\n",
        "        image_path = os.path.join(test_image_folder, filename)\n",
        "        if os.path.exists(image_path):\n",
        "            image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "            img_resized = cv2.resize(image, (640, 192))\n",
        "            test_images.append(img_resized)\n",
        "        else:\n",
        "            print(f\"Error: No file found for {filename}\")\n",
        "    return test_images\n",
        "\n",
        "def preprocess_images(images):\n",
        "    processed_images = []\n",
        "    for img in images:\n",
        "        processed_images.append(img_to_array(img))\n",
        "    return np.array(processed_images, dtype=np.float32) / 255.0\n",
        "\n",
        "\n",
        "# Load images\n",
        "test_images_left = load_data(\"/content/test/image_left\")\n",
        "test_images_right = load_data(\"/content/test/image_right\")  # Adjusted to load right images\n",
        "\n",
        "# Preprocess images\n",
        "test_images_left_processed = preprocess_images(test_images_left)\n",
        "# Predict using the preprocessed images\n",
        "predictions = model.predict(test_images_left_processed)\n",
        "\n",
        "for idx in range(30):\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "    # Display the original image\n",
        "    original_img = cv2.cvtColor(test_images_left[idx], cv2.COLOR_BGR2RGB)\n",
        "    axs[0].imshow(original_img)\n",
        "    axs[0].set_title('Original Image')\n",
        "    axs[0].axis('off')\n",
        "\n",
        "    # Prepare the overlay\n",
        "    predicted_mask = predictions[idx].squeeze()\n",
        "    overlay = np.zeros_like(original_img, dtype='uint8')  # match the data type\n",
        "    overlay[predicted_mask > 0.5] = [0, 255, 0]  # Green color in RGB\n",
        "\n",
        "    # Apply the overlay\n",
        "    overlay_image = cv2.addWeighted(original_img, 1, overlay, 0.5, 0)\n",
        "\n",
        "    # Display the prediction overlay on the original image\n",
        "    axs[1].imshow(overlay_image)\n",
        "    axs[1].set_title('Original Image with Road Prediction Overlay')\n",
        "    axs[1].axis('off')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d53e55d79f17fb5",
      "metadata": {
        "collapsed": false,
        "id": "6d53e55d79f17fb5"
      },
      "source": [
        "## 4. Fit a plane in 3D to the road pixels by using the depth of the pixels. Make sure your algorithm is robust to outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8Fizr6Yf4D_M",
      "metadata": {
        "id": "8Fizr6Yf4D_M"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import RANSACRegressor\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import RANSACRegressor\n",
        "\n",
        "def extract_3d_coordinates(depth_map, mask, focal_length, px, py):\n",
        "    # px, py are the principal points (usually the center of the image)\n",
        "    if mask.ndim == 3 and mask.shape[2] == 1:\n",
        "        mask = mask[:,:,0]  # This reduces it from (192, 640, 1) to (192, 640)\n",
        "\n",
        "    indices = np.where(mask > 0)\n",
        "    Z = depth_map[indices]\n",
        "    X = (indices[1] - px) * Z / focal_length\n",
        "    Y = (indices[0] - py) * Z / focal_length\n",
        "    return np.vstack((X, Y, Z)).T\n",
        "\n",
        "def fit_plane_RANSAC(points_3d):\n",
        "    # Fit a plane using RANSAC\n",
        "    ransac = RANSACRegressor()\n",
        "    ransac.fit(points_3d[:, :2], points_3d[:, 2])\n",
        "    return ransac\n",
        "\n",
        "def plot_3d(points_3d, plane_model):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.scatter(points_3d[:, 0], points_3d[:, 1], points_3d[:, 2], c='r', marker='o')\n",
        "\n",
        "    # Plot the plane\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "    X, Y = np.meshgrid(np.linspace(xlim[0], xlim[1], 10), np.linspace(ylim[0], ylim[1], 10))\n",
        "    Z = plane_model.predict(np.hstack((X.reshape(-1, 1), Y.reshape(-1, 1))))\n",
        "    Z = Z.reshape(X.shape)\n",
        "    ax.plot_surface(X, Y, Z, alpha=0.5)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vaTqIQaqf_Yx",
      "metadata": {
        "id": "vaTqIQaqf_Yx"
      },
      "outputs": [],
      "source": [
        "principal_point_x = 609.5593\n",
        "principal_point_y = 172.8540\n",
        "\n",
        "focal_length = 721.5377\n",
        "baseline = 0.5371505882506209\n",
        "\n",
        "# Compute depth maps\n",
        "depth_maps = [compute_depth_map(img_left, img_right, focal_length, baseline) for img_left, img_right in zip(test_images_left, test_images_right)]\n",
        "\n",
        "# Compute road masks\n",
        "road_masks = model.predict(test_images_left_processed)\n",
        "\n",
        "\n",
        "def plot_images(images, titles, cmap=None, figsize=(20, 10)):\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i, image in enumerate(images):\n",
        "        plt.subplot(1, len(images), i + 1)\n",
        "        plt.imshow(image, cmap=cmap)\n",
        "        plt.title(titles[i])\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "for i in range(10):\n",
        "  plot_images([depth_maps[i], road_masks[i]], ['Depth Map', 'Road Mask'], cmap='gray')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de826ec11cc86df2",
      "metadata": {
        "collapsed": false,
        "id": "de826ec11cc86df2"
      },
      "source": [
        "\n",
        "## 5. Plot each pixel in 3D (we call this a 3D point cloud). On the same plot, show also the estimated ground plane.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nCHHMWMdgJLL",
      "metadata": {
        "id": "nCHHMWMdgJLL"
      },
      "outputs": [],
      "source": [
        "# For each image\n",
        "i = 0\n",
        "for depth_map, mask in zip(depth_maps, road_masks):\n",
        "    points_3d = extract_3d_coordinates(depth_map, mask, focal_length, principal_point_x, principal_point_y)\n",
        "    plane_model = fit_plane_RANSAC(points_3d)\n",
        "    plot_3d(points_3d, plane_model)\n",
        "    if i == 10:\n",
        "        break\n",
        "    i += 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}